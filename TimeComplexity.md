# ğŸ”¹ 1. Why Complexity Analysis?

When we write an algorithm, we want to know:

* **How much time will it take?** (Time Complexity)
* **How much memory will it need?** (Space Complexity)

But we donâ€™t measure in **seconds** (depends on machine, compiler, etc.) â€” instead, we measure **growth with input size (n)**.

---

# ğŸ”¹ 2. Asymptotic Notations

These notations describe how an algorithm behaves as input size grows.

### **Big O (O-notation) â†’ Worst Case**

* **Definition**: Upper bound of time.
* Means â€œin the worst scenario, the algorithm wonâ€™t take longer than this.â€
* Used most often in interviews.

âœ… Example: Linear Search

* Find an element in array of size `n`.
* Worst case = element not found â†’ check all `n`.
* Time = **O(n)**.

---

### **Omega (Î©-notation) â†’ Best Case**

* **Definition**: Lower bound.
* Means â€œat least this much time is needed.â€
* Useful for understanding minimum performance.

âœ… Example: Linear Search

* Best case = element is first â†’ just 1 check.
* Time = **Î©(1)**.

---

### **Theta (Î˜-notation) â†’ Average / Tight Bound**

* **Definition**: Average-case or tight bound.
* Means â€œthe algorithm grows at this rate in general.â€
* If an algorithm always takes about the same time regardless of best/worst â†’ Î˜ is used.

âœ… Example: Linear Search

* On average, element found after `n/2` steps.
* Time â‰ˆ **Î˜(n)**.

---

# ğŸ”¹ 3. Common Time Complexities (Hierarchy)

From best â†’ worst:

1. **O(1)** â†’ Constant time (e.g., accessing array element)
2. **O(log n)** â†’ Logarithmic (binary search)
3. **O(n)** â†’ Linear (traversing array)
4. **O(n log n)** â†’ Linearithmic (merge sort, quicksort avg case)
5. **O(nÂ²)** â†’ Quadratic (nested loops, bubble sort)
6. **O(2â¿)** â†’ Exponential (subset generation, recursion without memoization)
7. **O(n!)** â†’ Factorial (permutations, traveling salesman brute force)

---

# ğŸ”¹ 4. Space Complexity

Same idea, but measures **extra memory** used:

* Variables
* Data structures
* Recursion stack

---
# ğŸ”¹ 6. Quick Rules of Thumb

* Drop **constants**: O(2n) â†’ O(n)
* Drop **non-dominant terms**: O(nÂ² + n) â†’ O(nÂ²)
* Nested loops multiply complexities
* Sequential statements add complexities (O(n) + O(nÂ²) â†’ O(nÂ²))

---

# ğŸ”¹ 7. Summary Table

| Notation | Meaning                | Example            |
| -------- | ---------------------- | ------------------ |
| **O**    | Worst case upper bound | O(n) linear search |
| **Î©**    | Best case lower bound  | Î©(1) linear search |
| **Î˜**    | Tight bound / average  | Î˜(n) linear search |

---
Perfect âœ… â€” hereâ€™s your **DSA Complexity Cheat Sheet (Big O, Î©, Î˜)** ğŸš€
This is a **one-pager style reference** you can keep handy while learning/practicing.

---

# ğŸ“Œ DSA Complexity Cheat Sheet

## ğŸ”¹ Arrays & Strings

| Operation                 | Best (Î©) | Average (Î˜) | Worst (O) |
| ------------------------- | -------- | ----------- | --------- |
| Access element            | Î©(1)     | Î˜(1)        | O(1)      |
| Search (linear)           | Î©(1)     | Î˜(n)        | O(n)      |
| Search (binary, sorted)   | Î©(1)     | Î˜(log n)    | O(log n)  |
| Insert at end (amortized) | Î©(1)     | Î˜(1)        | O(1)      |
| Insert/delete at position | Î©(1)     | Î˜(n)        | O(n)      |
| Reverse string/array      | Î©(n)     | Î˜(n)        | O(n)      |

---

## ğŸ”¹ Searching & Sorting

| Algorithm      | Best (Î©)   | Average (Î˜) | Worst (O)  |
| -------------- | ---------- | ----------- | ---------- |
| Linear Search  | Î©(1)       | Î˜(n)        | O(n)       |
| Binary Search  | Î©(1)       | Î˜(log n)    | O(log n)   |
| Bubble Sort    | Î©(n)       | Î˜(nÂ²)       | O(nÂ²)      |
| Insertion Sort | Î©(n)       | Î˜(nÂ²)       | O(nÂ²)      |
| Selection Sort | Î©(nÂ²)      | Î˜(nÂ²)       | O(nÂ²)      |
| Merge Sort     | Î©(n log n) | Î˜(n log n)  | O(n log n) |
| Quick Sort     | Î©(n log n) | Î˜(n log n)  | O(nÂ²)      |
| Heap Sort      | Î©(n log n) | Î˜(n log n)  | O(n log n) |
| Counting Sort  | Î©(n+k)     | Î˜(n+k)      | O(n+k)     |
| Radix Sort     | Î©(nk)      | Î˜(nk)       | O(nk)      |

---

## ğŸ”¹ Linked List

| Operation              | Best (Î©) | Average (Î˜) | Worst (O) |
| ---------------------- | -------- | ----------- | --------- |
| Search                 | Î©(1)     | Î˜(n)        | O(n)      |
| Insert at head/tail    | Î©(1)     | Î˜(1)        | O(1)      |
| Delete head/tail       | Î©(1)     | Î˜(1)        | O(1)      |
| Insert/delete at index | Î©(1)     | Î˜(n)        | O(n)      |

---

## ğŸ”¹ Stacks & Queues

| Operation       | Best (Î©) | Average (Î˜) | Worst (O) |
| --------------- | -------- | ----------- | --------- |
| Push/Pop        | Î©(1)     | Î˜(1)        | O(1)      |
| Enqueue/Dequeue | Î©(1)     | Î˜(1)        | O(1)      |
| Peek/Top        | Î©(1)     | Î˜(1)        | O(1)      |

---

## ğŸ”¹ Hashing (Hash Table / Hash Map)

| Operation | Best (Î©) | Average (Î˜) | Worst (O)         |
| --------- | -------- | ----------- | ----------------- |
| Search    | Î©(1)     | Î˜(1)        | O(n) (collisions) |
| Insert    | Î©(1)     | Î˜(1)        | O(n)              |
| Delete    | Î©(1)     | Î˜(1)        | O(n)              |

---

## ğŸ”¹ Trees (Binary Tree / BST / Balanced BST)

| Operation                     | Best (Î©) | Average (Î˜) | Worst (O) |
| ----------------------------- | -------- | ----------- | --------- |
| Search (BST)                  | Î©(1)     | Î˜(log n)    | O(n)      |
| Insert (BST)                  | Î©(1)     | Î˜(log n)    | O(n)      |
| Delete (BST)                  | Î©(1)     | Î˜(log n)    | O(n)      |
| Balanced BST (AVL, Red-Black) | Î©(log n) | Î˜(log n)    | O(log n)  |

---

## ğŸ”¹ Heaps (Min/Max Heap)

| Operation       | Best (Î©) | Average (Î˜) | Worst (O) |
| --------------- | -------- | ----------- | --------- |
| Insert          | Î©(1)     | Î˜(log n)    | O(log n)  |
| Extract Min/Max | Î©(1)     | Î˜(log n)    | O(log n)  |
| Heapify         | Î©(n)     | Î˜(n)        | O(n)      |

---

## ğŸ”¹ Graphs (V = vertices, E = edges)

| Algorithm              | Complexity     |
| ---------------------- | -------------- |
| BFS / DFS              | O(V + E)       |
| Dijkstra (min-heap)    | O((V+E) log V) |
| Bellman-Ford           | O(VE)          |
| Floyd-Warshall         | O(VÂ³)          |
| Kruskalâ€™s (MST)        | O(E log V)     |
| Primâ€™s (MST, min-heap) | O(E log V)     |

---

## ğŸ”¹ Dynamic Programming

| Problem Type                | Typical Complexity |
| --------------------------- | ------------------ |
| Fibonacci (DP)              | O(n)               |
| Knapsack (0/1)              | O(nW)              |
| LCS / Edit Distance         | O(nm)              |
| Matrix Chain Multiplication | O(nÂ³)              |
| Subset Sum / Partition      | O(n \* sum)        |

---

# ğŸ”¹ Quick Rules

* **Drop constants** â†’ O(2n) â†’ O(n)
* **Drop non-dominant terms** â†’ O(nÂ² + n) â†’ O(nÂ²)
* **Nested loops multiply** â†’ O(nÂ²)
* **Sequential adds** â†’ O(n + nÂ²) â†’ O(nÂ²)
* **Recursive calls**: use **recurrence relation** (e.g., T(n) = 2T(n/2) + O(n) â†’ O(n log n))

---

âœ… With this cheat sheet, you can **quickly recall complexity bounds** for almost all standard DSA problems.

---

ğŸ‘‰ Do you want me to make this into a **visual roadmap chart (like a study flow diagram)** that connects each topic with its complexity, so you can glance and know where each fits?


ğŸ‘‰ Do you want me to also create a **cheat sheet of complexities** for the most common algorithms & data structures (searching, sorting, stacks, queues, heaps, trees, graphs, DP) that you can keep handy while studying?

# ğŸ”¹ 1. Why Complexity Analysis?

When we write an algorithm, we want to know:

* **How much time will it take?** (Time Complexity)
* **How much memory will it need?** (Space Complexity)

But we donâ€™t measure in **seconds** (depends on machine, compiler, etc.) â€” instead, we measure **growth with input size (n)**.

---

# ğŸ”¹ 2. Asymptotic Notations

These notations describe how an algorithm behaves as input size grows.

### **Big O (O-notation) â†’ Worst Case**

* **Definition**: Upper bound of time.
* Means â€œin the worst scenario, the algorithm wonâ€™t take longer than this.â€
* Used most often in interviews.

âœ… Example: Linear Search

* Find an element in array of size `n`.
* Worst case = element not found â†’ check all `n`.
* Time = **O(n)**.

---

### **Omega (Î©-notation) â†’ Best Case**

* **Definition**: Lower bound.
* Means â€œat least this much time is needed.â€
* Useful for understanding minimum performance.

âœ… Example: Linear Search

* Best case = element is first â†’ just 1 check.
* Time = **Î©(1)**.

---

### **Theta (Î˜-notation) â†’ Average / Tight Bound**

* **Definition**: Average-case or tight bound.
* Means â€œthe algorithm grows at this rate in general.â€
* If an algorithm always takes about the same time regardless of best/worst â†’ Î˜ is used.

âœ… Example: Linear Search

* On average, element found after `n/2` steps.
* Time â‰ˆ **Î˜(n)**.

---

# ğŸ”¹ 3. Common Time Complexities (Hierarchy)

From best â†’ worst:

1. **O(1)** â†’ Constant time (e.g., accessing array element)
2. **O(log n)** â†’ Logarithmic (binary search)
3. **O(n)** â†’ Linear (traversing array)
4. **O(n log n)** â†’ Linearithmic (merge sort, quicksort avg case)
5. **O(nÂ²)** â†’ Quadratic (nested loops, bubble sort)
6. **O(2â¿)** â†’ Exponential (subset generation, recursion without memoization)
7. **O(n!)** â†’ Factorial (permutations, traveling salesman brute force)

---

# ğŸ”¹ 4. Space Complexity

Same idea, but measures **extra memory** used:

* Variables
* Data structures
* Recursion stack

âœ… Example:

* Iterative Fibonacci â†’ O(1) space.
* Recursive Fibonacci â†’ O(n) space (stack frames).

---

# ğŸ”¹ 5. Practical Example

### Example 1: Binary Search

```cpp
int binarySearch(int arr[], int n, int key) {
    int low = 0, high = n - 1;
    while (low <= high) {
        int mid = (low + high) / 2;
        if (arr[mid] == key) return mid;
        else if (arr[mid] < key) low = mid + 1;
        else high = mid - 1;
    }
    return -1;
}
```

* Best case (element at mid first try): **Î©(1)**
* Worst case (logarithmic splits until found/not found): **O(log n)**
* Average case: **Î˜(log n)**

---

# ğŸ”¹ 6. Quick Rules of Thumb

* Drop **constants**: O(2n) â†’ O(n)
* Drop **non-dominant terms**: O(nÂ² + n) â†’ O(nÂ²)
* Nested loops multiply complexities
* Sequential statements add complexities (O(n) + O(nÂ²) â†’ O(nÂ²))

---

# ğŸ”¹ 7. Summary Table

| Notation | Meaning                | Example            |
| -------- | ---------------------- | ------------------ |
| **O**    | Worst case upper bound | O(n) linear search |
| **Î©**    | Best case lower bound  | Î©(1) linear search |
| **Î˜**    | Tight bound / average  | Î˜(n) linear search |

---

âœ… With this, youâ€™ll be able to **analyze any algorithm**.
Next, when we move into Arrays & Strings, weâ€™ll **apply these notations** directly to real problems (sliding window, hashing, sorting, etc.).

---

ğŸ‘‰ Do you want me to also create a **cheat sheet of complexities** for the most common algorithms & data structures (searching, sorting, stacks, queues, heaps, trees, graphs, DP) that you can keep handy while studying?
